{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "\n",
    "\n",
    "training_file = 'wordstest.txt'\n",
    "\n",
    "\n",
    "#中文多文件\n",
    "def readalltxt(txt_files):\n",
    "    labels = []\n",
    "    for txt_file in txt_files:\n",
    "        \n",
    "        target = get_ch_lable(txt_file)\n",
    "        labels.append(target)  \n",
    "    return labels\n",
    "    \n",
    "#中文字\n",
    "def get_ch_lable(txt_file):  \n",
    "    labels= \"\"\n",
    "    with open(txt_file, 'rb') as f:\n",
    "        for label in f: \n",
    "            #labels =label.decode('utf-8')\n",
    "            labels =labels+label.decode('utf-8')\n",
    "           \n",
    "    return  labels\n",
    "    \n",
    "\n",
    "\n",
    "#优先转文件里的字符到向量\n",
    "def get_ch_lable_v(txt_file,word_num_map,txt_label=None):\n",
    "      \n",
    "    words_size = len(word_num_map)   \n",
    "    to_num = lambda word: word_num_map.get(word, words_size) \n",
    "    if txt_file!= None:\n",
    "        txt_label = get_ch_lable(txt_file)\n",
    "\n",
    "    labels_vector = list(map(to_num, txt_label)) \n",
    "    return labels_vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data...\n",
      "样本长度: 97\n",
      "字表大小: 69\n"
     ]
    }
   ],
   "source": [
    "training_data =get_ch_lable(training_file)\n",
    "\n",
    "print(\"Loaded training data...\")\n",
    "\n",
    "print('样本长度:',len(training_data))\n",
    "counter = Counter(training_data)  \n",
    "words = sorted(counter)\n",
    "words_size= len(words)\n",
    "word_num_map = dict(zip(words, range(words_size))) \n",
    "\n",
    "print('字表大小:', words_size)     \n",
    "wordlabel = get_ch_lable_v(training_file,word_num_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRURNN(torch.nn.Module):\n",
    "    def __init__(self, word_size, embed_dim,\n",
    "                 hidden_dim, output_size, num_layers):\n",
    "        super(GRURNN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embed = torch.nn.Embedding(word_size, embed_dim)\n",
    "        self.gru = torch.nn.GRU(input_size=embed_dim,\n",
    "                                hidden_size=hidden_dim,\n",
    "                                num_layers=num_layers,bidirectional=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim*2, output_size)\n",
    "\n",
    "    \n",
    "    def forward(self, features, hidden):\n",
    "        embedded = self.embed(features.view(1, -1))\n",
    "        output, hidden = self.gru(embedded.view(1, 1, -1), hidden)\n",
    "        output = self.fc(output.view(1, -1))\n",
    "        return output, hidden\n",
    "      \n",
    "    def init_zero_state(self):\n",
    "        init_hidden = torch.zeros(self.num_layers*2, 1, self.hidden_dim).to(DEVICE)\n",
    "        return init_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0001 min\n",
      "step 1000 | Loss 0.55\n",
      "\n",
      "\n",
      "心，前行的气力要坚持一种信念，默守一种精神，为自己积淀站立的信心，前行的 \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 0.0000 min\n",
      "step 2000 | Loss 0.01\n",
      "\n",
      "\n",
      "走，理想为我们关注了精神地蕴藉。所以生活再平凡、再普通、再琐碎，我们都要 \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 0.0001 min\n",
      "step 3000 | Loss 0.01\n",
      "\n",
      "\n",
      "通、再琐碎，我们都要坚持一种精神，为自己积淀站立的信心，前行的气力。所以 \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 0.0001 min\n",
      "step 4000 | Loss 0.78\n",
      "\n",
      "\n",
      "，默守一种精神地蕴藉。所以生活再平凡、再普通、再琐碎，我们都要坚持一种精 \n",
      "\n",
      "==================================================\n",
      "Time elapsed: 0.0001 min\n",
      "step 5000 | Loss 0.66\n",
      "\n",
      "\n",
      "，前行的气力要心头悬挂着远方的灯光，我们就会坚持不懈地走，理想为我们关注 \n",
      "\n",
      "==================================================\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_DIM = 20\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "model = GRURNN(words_size, EMBEDDING_DIM, HIDDEN_DIM, words_size, NUM_LAYERS)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def evaluate(model, prime_str, predict_len, temperature=0.8):\n",
    "\n",
    "    hidden = model.init_zero_state().to(DEVICE)\n",
    "    predicted = ''\n",
    "\n",
    "    #处理输入语义\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model(prime_str[p], hidden)\n",
    "        predicted +=words[prime_str[p]]\n",
    "    inp = prime_str[-1]\n",
    "    predicted +=words[inp]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model(inp, hidden)\n",
    "        \n",
    "        #从多项式分布中采样\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        inp = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        predicted += words[inp]\n",
    "\n",
    "    return predicted\n",
    "\n",
    "\n",
    "#定义参数训练模型\n",
    "training_iters = 5000\n",
    "display_step = 1000\n",
    "n_input = 4\n",
    "step = 0\n",
    "offset = random.randint(0,n_input+1)\n",
    "end_offset = n_input + 1\n",
    "\n",
    "while step < training_iters:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 随机取一个位置偏移\n",
    "    if offset > (len(training_data)-end_offset):\n",
    "        offset = random.randint(0, n_input+1)\n",
    "   \n",
    "\n",
    "    inwords =wordlabel[offset:offset+n_input]\n",
    "    inwords = np.reshape(np.array(inwords), [n_input, -1,  1])\n",
    "\n",
    "    out_onehot = wordlabel[offset+1:offset+n_input+1]\n",
    "\n",
    "\n",
    "    hidden = model.init_zero_state()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0.\n",
    "    inputs, targets = torch.LongTensor(inwords).to(DEVICE), torch.LongTensor(out_onehot).to(DEVICE)\n",
    "    for c in range(n_input):\n",
    "\n",
    "        outputs, hidden = model(inputs[c], hidden)\n",
    "        loss += F.cross_entropy(outputs, targets[c].view(1))\n",
    "\n",
    "    loss /= n_input\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    #输出日志\n",
    "    with torch.set_grad_enabled(False):\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(f'Time elapsed: {(time.time() - start_time)/60:.4f} min')\n",
    "            print(f'step {step+1} | Loss {loss.item():.2f}\\n\\n')\n",
    "            with torch.no_grad():\n",
    "                print(evaluate(model, inputs, 32), '\\n')\n",
    "            print(50*'=')\n",
    "\n",
    "    step += 1\n",
    "    offset += (n_input+1)#中间隔了一个，作为预测\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51a9663a131f1b5758c45b97a2d6917c8ae86b33e231c3733631cbc7265cfc89"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
